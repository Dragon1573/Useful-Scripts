{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "687d4428-758c-4ef9-9493-1601095f9ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 4, 5, 2, 1, 3])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 参考资料: https://www.jiqizhixin.com/articles/2024-02-16\n",
    "# 一、文本输入转为向量嵌入\n",
    "# 分词、转为token下标\n",
    "import torch\n",
    "sentence = 'Life is short, eat dessert first'\n",
    "dc = {s:i for i,s in enumerate(sorted(sentence.replace(',', '').split(' ')))}\n",
    "s_index = [dc[s] for s in sentence.replace(',', '').split(' ')]\n",
    "s_index = torch.tensor(s_index)\n",
    "s_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba38b19a-db1b-41c7-84e4-46be06cee15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3374, -0.1778, -0.3035],\n",
       "        [ 0.1794,  1.8951,  0.4954],\n",
       "        [ 0.2692, -0.0770, -1.0205],\n",
       "        [-0.2196, -0.3792,  0.7671],\n",
       "        [-0.5880,  0.3486,  0.6603],\n",
       "        [-1.1925,  0.6984, -1.4097]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对下标进行embed\n",
    "vocab_size = 50_000\n",
    "torch.manual_seed(123)\n",
    "embed = torch.nn.Embedding(vocab_size, 3)\n",
    "embeded_sentence = embed(s_index).detach()\n",
    "embeded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fe94023-1935-4274-bcc5-61f323dda914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0327, -0.2112],\n",
      "        [ 0.5667,  1.8269],\n",
      "        [-0.0152, -0.7982],\n",
      "        [-0.1037,  0.2902],\n",
      "        [-0.0375,  0.5085],\n",
      "        [-0.2816, -1.3567]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 二、自注意力机制\n",
    "# 初始化q、k、v的权重矩阵\n",
    "torch.manual_seed(123)\n",
    "d = embeded_sentence.shape[1]\n",
    "d_q, d_k, d_v = 2, 2, 4\n",
    "\n",
    "w_q = torch.nn.Parameter(torch.rand(d, d_q))\n",
    "w_k = torch.nn.Parameter(torch.rand(d, d_k))\n",
    "w_v = torch.nn.Parameter(torch.rand(d, d_v))\n",
    "print(embeded_sentence @ w_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b55c9e99-94cc-4332-b68a-4ffba8e24b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0152, -0.7982], grad_fn=<SqueezeBackward4>)\n",
      "tensor([[ 0.0327, -0.2112],\n",
      "        [ 0.5667,  1.8269],\n",
      "        [-0.0152, -0.7982],\n",
      "        [-0.1037,  0.2902],\n",
      "        [-0.0375,  0.5085],\n",
      "        [-0.2816, -1.3567]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.0823, -0.3031],\n",
      "        [ 0.5295,  1.7355],\n",
      "        [-0.2991, -0.7295],\n",
      "        [ 0.1420,  0.2291],\n",
      "        [ 0.1920,  0.6467],\n",
      "        [-0.4788, -0.5835]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.2546, -0.2608, -0.1544, -0.2801],\n",
      "        [ 0.6612,  1.8972,  1.0963,  1.8106],\n",
      "        [-0.8598, -0.6161, -0.5940, -0.9455],\n",
      "        [ 0.5932,  0.0981,  0.2741,  0.4151],\n",
      "        [ 0.5605,  0.5645,  0.3676,  0.6429],\n",
      "        [-1.2107, -0.4929, -1.0081, -1.4031]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 计算q、k、v\n",
    "x_2 = embeded_sentence[2]\n",
    "q_2 = x_2 @ w_q\n",
    "k_2 = x_2 @ w_k\n",
    "v_2 = x_2 @ w_v\n",
    "print(q_2)\n",
    "querys = embeded_sentence @ w_q\n",
    "keys = embeded_sentence @ w_k\n",
    "values = embeded_sentence @ w_v\n",
    "print(querys)\n",
    "print(keys)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f4a2814-4469-4100-80f0-79d82acd626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.5191, grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 注意力权重ω(i,j) 是查询和键序列之间的点积 ω(i,j) = q⁽ⁱ⁾ k⁽ʲ⁾\n",
    "omega_24 = q_2.dot(keys[4])\n",
    "print(omega_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f15509b6-2e68-4146-9d85-df2a82bc8e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2432, -1.3934,  0.5869, -0.1851, -0.5191,  0.4730],\n",
       "       grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 例：计算第三个词对整个序列的注意力权重 w, omega\n",
    "omega_2 = q_2 @ keys.T\n",
    "omega_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67a1ff6f-909c-4f7c-b5d7-f0e938e7bb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1965, 0.0618, 0.2506, 0.1452, 0.1146, 0.2312],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 归一化\n",
    "import torch.nn.functional as F\n",
    "attention_w_2 = F.softmax(omega_2/d_k ** 0.5, dim=0)\n",
    "print(attention_w_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47d906a2-8ce4-45a2-bd50-23c520429719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3542, -0.1234, -0.2627, -0.3706], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用归一化后的注意力，计算上下文向量嵌入\n",
    "context_vec_2 = attention_w_2 @ values\n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ae9d3a5-9991-449b-b944-546062af6324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将自注意力融合为一个类\n",
    "import torch.nn as nn\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v):\n",
    "        super().__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        self.w_query = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.w_key = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.w_value = nn.Parameter(torch.rand(d_in, d_out_v))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keys = x @ self.w_key\n",
    "        queries = x @ self.w_query\n",
    "        values = x @ self.w_value\n",
    "        attn_scores = queries @ keys.T\n",
    "        # 得到归一化的，每个token彼此之间的注意力值，seq_length * seq_length\n",
    "        attn_weights = torch.softmax(attn_scores/self.d_out_kq ** 0.5, dim=-1)\n",
    "        # 得到在每一个value维度上，每个token使用自己与其他token的注意力 @ 该维度的value , seq_length * d_v\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "692b397c-eb2f-4cd3-a91e-301593663a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1564,  0.1028, -0.0763, -0.0764],\n",
      "        [ 0.5313,  1.3607,  0.7891,  1.3110],\n",
      "        [-0.3542, -0.1234, -0.2627, -0.3706],\n",
      "        [ 0.0071,  0.3345,  0.0969,  0.1998],\n",
      "        [ 0.1008,  0.4780,  0.2021,  0.3674],\n",
      "        [-0.5296, -0.2799, -0.4107, -0.6006]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 测试，结果中的第三行与上文计算的上下文嵌入一致\n",
    "torch.manual_seed(123)\n",
    "\n",
    "d_in, d_out_kq, d_out_v = 3, 2, 4\n",
    "\n",
    "sa = SelfAttention(d_in, d_out_kq, d_out_v)\n",
    "print(sa(embeded_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d9d8261-4b19-4c61-a7c1-5864797e3978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 三、多头注意力\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v, num_heads):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SelfAttention(d_in, d_out_kq, d_out_v) for _ in range(num_heads)])\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d73c9284-76a0-4519-b6a8-1fb6af8ee839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0185],\n",
      "        [ 0.4003],\n",
      "        [-0.1103],\n",
      "        [ 0.0668],\n",
      "        [ 0.1180],\n",
      "        [-0.1827]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.0185,  0.0170,  0.1999, -0.0860],\n",
      "        [ 0.4003,  1.7137,  1.3981,  1.0497],\n",
      "        [-0.1103, -0.1609,  0.0079, -0.2416],\n",
      "        [ 0.0668,  0.3534,  0.2322,  0.1008],\n",
      "        [ 0.1180,  0.6949,  0.3157,  0.2807],\n",
      "        [-0.1827, -0.2060, -0.2393, -0.3167]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 举例测试多头注意力机制\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# 单头注意力，输出维度为seq_len * d_v\n",
    "d_in, d_out_kq, d_out_v = 3, 2, 1\n",
    "sa = SelfAttention(d_in, d_out_kq, d_out_v)\n",
    "print(sa(embeded_sentence))\n",
    "\n",
    "# 多头注意力，在最后一个维度拼接，输出维度为 seq_len * (d_v * num_heads)\n",
    "torch.manual_seed(123)\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out_kq, d_out_v, num_heads=4)\n",
    "context_vecs = mha(embeded_sentence)\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c6df9cb-f33b-4e32-b792-77704a4b9c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 四、交叉注意力(从selfattention的基础上改)\n",
    "import torch.nn as nn\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out_kq, d_out_v):\n",
    "        super().__init__()\n",
    "        self.d_out_kq = d_out_kq\n",
    "        self.w_query = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.w_key = nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "        self.w_value = nn.Parameter(torch.rand(d_in, d_out_v))\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        queries = x2 @ self.w_query\n",
    "        \n",
    "        keys = x1 @ self.w_key\n",
    "        values = x1 @ self.w_value\n",
    "        attn_scores = queries @ keys.T\n",
    "        # 得到归一化的，每个x2的token对每个x1的token之间的注意力值，输出维度x2_seq_length * x1_seq_length\n",
    "        attn_weights = torch.softmax(attn_scores/self.d_out_kq ** 0.5, dim=-1)\n",
    "        # 得到在每一个value维度上，每个x2的token对每个x1的token之间的注意力值 @ x1的该token在该维度的value \n",
    "        # 输出维度为x2_seq_length * d_v，代表从value的不同维度上（角度/语境）对应不同注意力值，计算上下文的嵌入\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f25a946-381a-4125-9fbc-5aad1d2825f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3374, -0.1778, -0.3035],\n",
      "        [ 0.1794,  1.8951,  0.4954],\n",
      "        [ 0.2692, -0.0770, -1.0205],\n",
      "        [-0.2196, -0.3792,  0.7671],\n",
      "        [-0.5880,  0.3486,  0.6603],\n",
      "        [-1.1925,  0.6984, -1.4097]])\n",
      "tensor([[0.2745, 0.6584, 0.2775],\n",
      "        [0.8573, 0.8993, 0.0390],\n",
      "        [0.9268, 0.7388, 0.7179],\n",
      "        [0.7058, 0.9156, 0.4340],\n",
      "        [0.0772, 0.3565, 0.1479],\n",
      "        [0.5331, 0.4066, 0.2318],\n",
      "        [0.4545, 0.9737, 0.4606],\n",
      "        [0.5159, 0.4220, 0.5786]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "d_in, d_out_kq, d_out_v = 3, 2, 4\n",
    "cat = CrossAttention(d_in, d_out_kq, d_out_v)\n",
    "\n",
    "x1 = embeded_sentence\n",
    "x2 = torch.rand(8, d_in)\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8aad73b9-9b8a-4200-b930-3a21c59f0c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2628, 0.7515, 0.3963, 0.6775],\n",
      "        [0.3689, 0.9600, 0.5367, 0.9030],\n",
      "        [0.4914, 1.2517, 0.7219, 1.2023],\n",
      "        [0.4381, 1.1187, 0.6384, 1.0672],\n",
      "        [0.0906, 0.4545, 0.1880, 0.3441],\n",
      "        [0.2374, 0.7029, 0.3635, 0.6248],\n",
      "        [0.4167, 1.0701, 0.6070, 1.0166],\n",
      "        [0.3376, 0.8998, 0.4955, 0.8371]], grad_fn=<MmBackward0>)\n",
      "torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "context_vecs = cat(x1, x2)\n",
    "print(context_vecs)\n",
    "print(context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d68e27b1-d2e7-44d1-9772-c8052db13361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0613, -0.3491,  0.1443, -0.0437, -0.1303,  0.1076],\n",
      "        [-0.6004,  3.4707, -1.5023,  0.4991,  1.2903, -1.3374],\n",
      "        [ 0.2432, -1.3934,  0.5869, -0.1851, -0.5191,  0.4730],\n",
      "        [-0.0794,  0.4487, -0.1807,  0.0518,  0.1677, -0.1197],\n",
      "        [-0.1510,  0.8626, -0.3597,  0.1112,  0.3216, -0.2787],\n",
      "        [ 0.4344, -2.5037,  1.0740, -0.3509, -0.9315,  0.9265]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 五、掩码自注意力（因果自注意力，Causal self-attention）\n",
    "# 回顾自注意力\n",
    "torch.manual_seed(123)\n",
    "\n",
    "d_in, d_out_kq, d_out_v = 3, 2, 4\n",
    "\n",
    "w_q = torch.nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "w_k = torch.nn.Parameter(torch.rand(d_in, d_out_kq))\n",
    "w_v = torch.nn.Parameter(torch.rand(d_in, d_out_v))\n",
    "\n",
    "x = embeded_sentence\n",
    "\n",
    "q = x @ w_q\n",
    "k = x @ w_k\n",
    "atten_scores = q @ k.T\n",
    "print(atten_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bacc11bb-a215-4aeb-acfd-935c46e7e23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1772, 0.1326, 0.1879, 0.1645, 0.1547, 0.1831],\n",
       "        [0.0386, 0.6870, 0.0204, 0.0840, 0.1470, 0.0229],\n",
       "        [0.1965, 0.0618, 0.2506, 0.1452, 0.1146, 0.2312],\n",
       "        [0.1505, 0.2187, 0.1401, 0.1651, 0.1793, 0.1463],\n",
       "        [0.1347, 0.2758, 0.1162, 0.1621, 0.1881, 0.1231],\n",
       "        [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_weights = torch.softmax(atten_scores/d_out_kq**0.5, dim=1)\n",
    "atten_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5c969d2-4458-4777-b785-642b47ae6e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用torch.tril构建简单掩码矩阵\n",
    "block_size = atten_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(block_size, block_size))\n",
    "mask_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "946e38df-0805-458d-ae89-4df2fe5221c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1772, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0386, 0.6870, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1965, 0.0618, 0.2506, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1505, 0.2187, 0.1401, 0.1651, 0.0000, 0.0000],\n",
       "        [0.1347, 0.2758, 0.1162, 0.1621, 0.1881, 0.0000],\n",
       "        [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用*构建掩码注意力\n",
    "masked_atten = atten_weights * mask_simple\n",
    "masked_atten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4f04b4f-2fa1-4636-91e2-f31f8dc447f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0532, 0.9468, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3862, 0.1214, 0.4924, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2232, 0.3242, 0.2078, 0.2449, 0.0000, 0.0000],\n",
      "        [0.1536, 0.3145, 0.1325, 0.1849, 0.2145, 0.0000],\n",
      "        [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor([[-0.2546, -0.2608, -0.1544, -0.2801],\n",
      "        [ 0.6612,  1.8972,  1.0963,  1.8106],\n",
      "        [-0.8598, -0.6161, -0.5940, -0.9455],\n",
      "        [ 0.5932,  0.0981,  0.2741,  0.4151],\n",
      "        [ 0.5605,  0.5645,  0.3676,  0.6429],\n",
      "        [-1.2107, -0.4929, -1.0081, -1.4031]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.2546, -0.2608, -0.1544, -0.2801],\n",
      "        [ 0.6124,  1.7823,  1.0298,  1.6994],\n",
      "        [-0.4415, -0.1738, -0.2191, -0.3539],\n",
      "        [ 0.1242,  0.4529,  0.2647,  0.4297],\n",
      "        [ 0.2848,  0.6142,  0.3719,  0.6158],\n",
      "        [-0.5296, -0.2799, -0.4107, -0.6006]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 再度进行归一化，逐行、注意力保持和为1\n",
    "row_sums = masked_atten.sum(dim=1, keepdim=True)\n",
    "masked_atten_norm = masked_atten / row_sums\n",
    "print(masked_atten_norm)\n",
    "\n",
    "# 使用掩码注意力计算上下文嵌入\n",
    "v = x @ w_v\n",
    "print(v)\n",
    "masked_context_vec = masked_atten_norm @ v\n",
    "print(masked_context_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59f5c460-6e4c-4a0d-b299-e5f8a2d9d8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True,  True],\n",
      "        [False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False,  True,  True],\n",
      "        [False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False]])\n",
      "tensor([[ 0.0613,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.6004,  3.4707,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.2432, -1.3934,  0.5869,    -inf,    -inf,    -inf],\n",
      "        [-0.0794,  0.4487, -0.1807,  0.0518,    -inf,    -inf],\n",
      "        [-0.1510,  0.8626, -0.3597,  0.1112,  0.3216,    -inf],\n",
      "        [ 0.4344, -2.5037,  1.0740, -0.3509, -0.9315,  0.9265]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0168, 0.9832, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3839, 0.0747, 0.5414, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2110, 0.3578, 0.1907, 0.2406, 0.0000, 0.0000],\n",
      "        [0.1338, 0.3688, 0.1086, 0.1740, 0.2147, 0.0000],\n",
      "        [0.1888, 0.0100, 0.3580, 0.0861, 0.0482, 0.3089]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 六、掩码自注意力更高效的实现方法\n",
    "# 将上述 【计算注意力分数-》softmax权重-》掩码注意力-》归一化】 的过程使用 【负无穷掩码-》softmax】的方法实现\n",
    "mask = torch.triu(torch.ones(block_size, block_size), diagonal=1)\n",
    "print(mask.bool())\n",
    "masked_atten = atten_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked_atten)\n",
    "masked_atten_soft = torch.softmax(masked_atten, dim=1)\n",
    "print(masked_atten_soft)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
